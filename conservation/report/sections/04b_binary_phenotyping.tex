\subsection{Binary phenotyping: the challenge assay problem}
\label{subsec:binary_phenotyping}

The theory above assumes we can measure resistance as a 
continuous value. In practice, we cannot. The only feasible 
phenotyping method is a \textbf{challenge assay}: expose an 
individual to \textit{Vibrio pectenicida} and observe whether 
it lives or dies. This produces a binary outcome, not a 
continuous trait measurement.

This fundamentally limits our ability to detect and select 
for genetic variation.

\subsubsection{The observation model}

For an individual with traits $(r_i, t_i, c_i)$, the 
probability of surviving a single challenge exposure is 
(\Cref{eq:fitness_function}):
\begin{equation}
    p_{\text{surv},i} = r_i + (1 - r_i) \cdot s(t_i, c_i)
    \label{eq:survival_prob}
\end{equation}

What we observe is a Bernoulli draw:
\begin{equation}
    Y_i \sim \text{Bernoulli}(p_{\text{surv},i})
    \label{eq:observed_outcome}
\end{equation}

This introduces two fundamental problems:

\textbf{Problem 1: False negatives.} A genetically resistant 
individual ($r_i = 0.40$, say) may still die in a single 
challenge trial. With $s(t,c) \approx 0.002$ at population 
mean tolerance/recovery:
\begin{equation}
    p_{\text{surv}}(r = 0.40) \approx 0.40 + 0.60 \times 0.002 
    = 0.401
\end{equation}
So even a ``good'' individual has only a 40\% chance of 
surviving a single exposure. We lose 60\% of our best 
genetics to stochastic disease.

\textbf{Problem 2: False positives.} A genetically average 
individual ($r_i = 0.15$) has:
\begin{equation}
    p_{\text{surv}}(r = 0.15) \approx 0.15 + 0.85 \times 0.002 
    = 0.152
\end{equation}
About 15\% of average individuals survive — not because they're 
genetically special, but because they got lucky (stochastic 
infection avoidance).

\subsubsection{Heritability on the observed scale}

This connects to the classical \textbf{threshold model} of 
quantitative genetics \citep{falconer1996introduction}. The 
binary phenotype (survive/die) is determined by an underlying 
continuous ``liability'' (here, $p_{\text{surv}}$). The 
heritability on the binary (observed) scale relates to the 
heritability on the underlying (liability) scale:
\begin{equation}
    h^2_{\text{obs}} = h^2_{\text{liability}} \cdot 
    \frac{z^2}{P(1 - P)}
    \label{eq:h2_observed}
\end{equation}
where $P = \E[p_{\text{surv}}]$ is the population survival 
rate and $z = \phi(\Phi^{-1}(P))$ is the standard normal 
ordinate at the threshold.

For our model with $h^2_{\text{liability}} = 1$ and 
$P \approx 0.15$ (population mean survival):
\begin{equation}
    z = \phi(\Phi^{-1}(0.15)) = \phi(1.04) = 0.231
\end{equation}
\begin{equation}
    h^2_{\text{obs}} = 1.0 \times \frac{0.231^2}{0.15 \times 0.85}
    = \frac{0.0534}{0.1275} = 0.419
\end{equation}

\note{The effective heritability drops from 1.0 to $\sim$0.42 
simply because we observe a binary outcome instead of the 
continuous trait. This means \textbf{selection response is 
roughly halved} compared to what we'd get with perfect 
trait measurement.}

\subsubsection{Selection differential from binary screening}

When we select survivors of a challenge assay, the selection 
differential on the underlying liability scale is:
\begin{equation}
    S_{\text{liability}} = \frac{z}{P} \cdot \sigma_{\text{liability}}
    \label{eq:s_binary}
\end{equation}

And the response on the liability scale is:
\begin{equation}
    R_{\text{liability}} = h^2_{\text{liability}} \cdot 
    \frac{z}{P} \cdot \sigma_{\text{liability}}
    \label{eq:r_binary}
\end{equation}

For our model ($\sigma_{\text{liability}} = \sigma_r \approx 0.078$, 
$P = 0.15$, $z = 0.231$):
\begin{equation}
    R \approx 1.0 \times \frac{0.231}{0.15} \times 0.078 
    = 0.120
\end{equation}

This predicts a per-generation gain of $\Delta r \approx 0.12$ 
under binary selection. Compare to truncation selection at 
$p = 0.15$ on the continuous trait:
\begin{equation}
    R_{\text{continuous}} = i_{p=0.15} \times \sigma_r 
    = 1.55 \times 0.078 = 0.121
\end{equation}

Interestingly, these are similar — because at $P = 0.15$, 
challenge-assay selection is equivalent to truncation selection 
keeping the top 15\%, which has selection intensity $i = 1.55$. 
The binary outcome \emph{happens to be} a reasonable proxy 
for the continuous trait when survival is dominated by 
resistance (since $r_i$ is the primary determinant of 
$p_{\text{surv}}$).

\subsubsection{The noise problem worsens at higher resistance}

As the population mean resistance increases through breeding, 
the signal-to-noise ratio of the challenge assay \emph{worsens}:

\begin{center}
\begin{tabular}{cccc}
\toprule
Population $\bar{r}$ & Survival rate $P$ & 
$h^2_{\text{obs}}$ & Information quality \\
\midrule
0.15 & 15\% & 0.42 & Moderate \\
0.30 & 30\% & 0.25 & Low \\
0.50 & 50\% & 0.21 & Low \\
0.70 & 70\% & 0.25 & Low \\
0.90 & 90\% & 0.42 & Moderate (but few die) \\
\bottomrule
\end{tabular}
\end{center}

At intermediate resistance ($\bar{r} \approx 0.3$--$0.7$), 
the challenge assay becomes a poor discriminator: too many 
average individuals survive (false positives) and too many 
good individuals die (false negatives). This is the 
\textbf{valley of low information} for binary phenotyping.

\subsubsection{Repeated exposures}

One way to improve discrimination is to expose survivors to 
\emph{multiple} sequential challenges. A survivor of $k$ 
independent exposures has an effective survival probability:
\begin{equation}
    p_{\text{surv}}^{(k)} = (p_{\text{surv},i})^k
    \label{eq:repeated_exposure}
\end{equation}

This amplifies the difference between resistant and susceptible 
individuals:

\begin{center}
\begin{tabular}{cccc}
\toprule
$r_i$ & $p_{\text{surv}}^{(1)}$ & 
$p_{\text{surv}}^{(2)}$ & $p_{\text{surv}}^{(3)}$ \\
\midrule
0.10 & 10.2\% & 1.0\% & 0.1\% \\
0.20 & 20.2\% & 4.1\% & 0.8\% \\
0.30 & 30.1\% & 9.1\% & 2.7\% \\
0.40 & 40.1\% & 16.1\% & 6.5\% \\
0.50 & 50.1\% & 25.1\% & 12.6\% \\
\bottomrule
\end{tabular}
\end{center}

After 3 exposures, an individual with $r = 0.40$ has 6.5\% 
survival, while $r = 0.20$ has 0.8\% — an 8$\times$ difference, 
compared to only 2$\times$ from a single exposure.

\textbf{The cost}: you lose many individuals, including 
genetically valuable ones. After 3 exposures, even your 
$r = 0.50$ individuals have only 12.6\% survival. This is 
an ethical and practical constraint — you're deliberately 
killing most of your captive population to identify the best.

\subsubsection{Implications for breeding program design}

\begin{enumerate}
    \item \textbf{Larger founder pools needed.} Because the 
    assay is noisy, you need more individuals to reliably 
    identify the best. The required sample sizes in 
    \Cref{sec:screening} should be inflated by a factor of 
    approximately $1/h^2_{\text{obs}} \approx 2$--$4\times$.
    
    \item \textbf{Family-based inference helps.} If you can 
    challenge 100 offspring from a single cross and observe 
    50\% survival (vs.\ population average of 15\%), that 
    cross likely has above-average resistance — even though 
    individual outcomes are noisy. \textbf{Family mean 
    survival rate} is a better estimator of parental 
    breeding value than individual binary outcome.
    
    \item \textbf{Genomic markers would be transformative.} 
    If the Schiebelhut GWAS loci can be validated as 
    resistance markers, non-lethal genotyping replaces 
    lethal challenge assays. This would:
    \begin{itemize}
        \item Eliminate false negatives (no good stars lost)
        \item Provide continuous measurement (full trait 
        resolution)
        \item Enable screening without killing
        \item Allow within-family selection without 
        sacrificing offspring
    \end{itemize}
    Validating these markers should be a high priority 
    for the conservation program.
    
    \item \textbf{Repeated exposure is powerful but costly.}
    Two exposures roughly double the information content 
    while halving the surviving pool. The optimal number 
    of exposures depends on the value of information vs.\ 
    the cost of losing individuals.
    
    \item \textbf{The R $\to$ S biology helps.} Because 
    recovered individuals return to the susceptible pool, 
    survivors of a first exposure can be re-challenged 
    without waiting for a new generation. This enables 
    sequential screening within a single cohort.
\end{enumerate}

\subsubsection{Required modification to screening theory}

The screening effort calculations in \Cref{sec:screening} 
assumed we can identify individuals above a resistance 
threshold. With binary phenotyping, we must replace the 
exceedance probability with the \textbf{survival probability}:

\begin{equation}
    p_{\text{screen}} = \E[p_{\text{surv}}] = 
    \E[r + (1-r) \cdot s(t,c)]
    \label{eq:screening_binary}
\end{equation}

and the ``threshold'' becomes implicit: we select whoever 
survives, regardless of their actual trait values. The 
expected resistance among survivors is:
\begin{equation}
    \E[r \mid \text{survived}] = 
    \frac{\E[r \cdot p_{\text{surv}}(r)]}
    {\E[p_{\text{surv}}(r)]}
    \label{eq:expected_r_survivors}
\end{equation}

This is computed numerically from the joint distribution of 
$(r, t, c)$ and will be a core output of the calibrated model.
